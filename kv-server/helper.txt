Great—your setup already supports this. You just need to **drive it** with the right workloads and a couple of tiny knobs so one case pounds the **disk/DB (I/O-bound)** and another hammers the **CPU (cache/HTTP path)**.

Below is a concrete, step-by-step plan with exact commands and what you should observe.

---

# 1) Prep: run the server with a sensible cache

Start the server in one terminal:

```bash
# from repo root or build/
./kv-server --port 8080 --cache-size 10000 --threads 8 --db kv_store.db
```

* A large cache lets you create a cache-hit workload later.
* Threads 8 is fine to start (bump to 16–32 if you have more cores).

---

# 2) Workload A — I/O-bound (two flavors)

## A1) **put-all** (write-heavy, disk-bound)

Writes always hit SQLite (even if cached), so this drives the disk/DB.

```bash
# many writers, 30s, only PUTs
./kv-loadgen --clients 64 --duration 30s --workload put-all --timeout-ms 3000
```

### What you should see

* **Throughput**: lower than the CPU-bound case.
* **Latency**: noticeably higher p95/p99.
* **System**: `iostat -dx 1` shows high util% on the device holding `kv_store.db`.

  * Also try: `vmstat 1`, `pidstat -d 1 -p $(pgrep kv-server | head -1)`

### Dial it to *more* I/O-bound

* In `src/database.cpp` change:

  ```cpp
  exec_sql(g_db, "PRAGMA synchronous=NORMAL;");
  ```

  to

  ```cpp
  exec_sql(g_db, "PRAGMA synchronous=FULL;");
  ```

  Rebuild and rerun. `FULL` adds extra fsyncs → even more disk-bound.

---

## A2) **get-all** unique reads (cache-miss, DB-bound)

Make the cache **miss** on purpose by reading a huge unique key space.

```bash
# make the hot set larger than cache to force misses
./kv-loadgen --clients 64 --duration 30s --workload get-all --timeout-ms 3000
```

### What you should see

* **Throughput**: lower than get-popular (below).
* **Latency**: higher/tailier than CPU-bound case.
* **System**: modest CPU %, **noticeable I/O**. `iostat` shows reads.

> Tip: You can prefill the DB so reads return 200 instead of 404 (either way they still hit DB). For quick prefill:
>
> ```bash
> for i in $(seq 1 200000); do ./kv-client put "k$i" "v"; done
> ```
>
> (Stop early if too slow; it’s just to get a big keyspace.)

---

# 3) Workload B — CPU-bound (**get-popular**, cache-hit heavy)

Drive requests against a **small hot set** that fits in cache to avoid DB. That keeps the server in memory/CPU and HTTP parsing/serialization.

```bash
# choose a hot set << cache_size so almost every GET hits the cache
./kv-loadgen --clients 128 --duration 30s --workload get-popular --keys 500
```

### What you should see

* **Throughput**: much higher than the I/O-bound runs.
* **Latency**: much lower (tight p50/p95).
* **System**: high **CPU%** on the `kv-server` process; `iostat` shows low disk activity.

  * Use: `top` or `htop`, `mpstat -P ALL 1`.

### Make it *more* CPU-bound

* Increase `--clients` (e.g., 256 or 512) to push the HTTP and cache path harder.
* Increase `--threads` in server start (e.g., `--threads 32`) to let it keep more requests in flight.
* Keep `--keys` well below `--cache-size` to stay near-100% hit rate.

---

# 4) Mixed workload (balanced)

If you want a middle ground with tunable mix:

```bash
./kv-loadgen --clients 64 --duration 30s \
  --workload mixed --keys 1000 --put-ratio 0.10 --delete-ratio 0.05
```

* Increase `--put-ratio` to shift toward I/O-bound.
* Decrease both ratios and keep `--keys` small to approach CPU-bound.

---

# 5) Verify bottlenecks (quick tools)

Open another terminal while each run is active:

* **CPU focus**:

  ```bash
  top -p $(pgrep -d, kv-server)
  mpstat -P ALL 1
  ```
* **Disk focus**:

  ```bash
  iostat -dx 1
  pidstat -d 1 -p $(pgrep kv-server | head -1)
  ```
* **Context switching / run queue**:

  ```bash
  vmstat 1
  ```

Expectations:

* CPU-bound run: `kv-server` uses lots of CPU; disks mostly idle.
* I/O-bound runs: disk util% and await times increase; CPU is not saturated.

---

# 6) Small knobs to accentuate differences

* **Cache size**:

  * CPU-bound: `--cache-size` >> `--keys`.
  * I/O-bound: `--cache-size` << working set (or use `get-all`).

* **SQLite durability**:

  * More I/O-bound: `PRAGMA synchronous=FULL;` (as shown above).
  * Less I/O-bound (faster but less durable): keep `NORMAL`.

* **Logging**:

  * Set `log_level` to `"ERROR"` in `config/server_config.json` to remove log overhead during load tests.

* **Binary flags**:

  * Build with `-DENABLE_SANITIZERS=OFF` for clean perf runs.
  * Ensure `-O2`/`-O3` (you already have defaults via release builds if you set `CMAKE_BUILD_TYPE=Release`).

  ```bash
  rm -rf build
  cmake -S . -B build -DBUILD_TESTS=OFF -DCMAKE_BUILD_TYPE=Release
  cmake --build build -j"$(nproc)"
  ```

---

# 7) (Optional) Add a simple `/metrics` to show hit/miss

If you want on-page evidence, add two atomics in `server.cpp` and increment on cache hit/miss. Then expose a `/metrics` text endpoint. (Totally optional, but handy to prove cache behavior during demos.)

---

## TL;DR commands

**CPU-bound (cache-hit heavy)**

```bash
./kv-server --port 8080 --cache-size 20000 --threads 16
./kv-loadgen --clients 128 --duration 30s --workload get-popular --keys 500
```

**I/O-bound (DB-heavy)**

```bash
# writes (disk-bound)
./kv-loadgen --clients 64 --duration 30s --workload put-all

# or cache-miss reads (DB-bound)
./kv-loadgen --clients 64 --duration 30s --workload get-all
```

Measure with `top`, `iostat -dx 1`, `mpstat -P ALL 1`.
You’ll clearly see one run saturate CPU and the other saturate disk/DB.
